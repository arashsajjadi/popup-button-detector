# ğŸš€ Pop-up Button Detector (AI-Powered)

## **ğŸ“Œ Introduction**
This project provides an **AI-powered system to automatically detect and locate pop-up buttons** in UI screenshots (such as "Not Now", "OK", "Cancel", etc.). It combines **YOLO for object detection, EasyOCR for text extraction, and LLaVA for text-based decision-making**.

The model is designed to work with **mobile UI screenshots** and can be fine-tuned to detect different types of buttons based on a dataset.

---

## **âš™ï¸ Features**
- **YOLO Object Detection** ğŸ–¼ï¸: Identifies UI elements like buttons using a pre-trained YOLO model.
- **EasyOCR Text Recognition** ğŸ” : Extracts text from images to identify button labels.
- **LLaVA Language Model** ğŸ§ : Determines which button corresponds to the "dismiss" action.
- **Auto-Clicking Logic** ğŸ–±ï¸: Computes the exact **click coordinates** of the target button.
- **Customizable and Fine-Tunable** ğŸ”§: YOLO can be fine-tuned for better detection performance.

---

## **ğŸ“‚ Project Structure**
```
ğŸ“¦ popup-button-detector
â”‚â”€â”€ ğŸ“„ popup_button_detector.ipynb  # Jupyter Notebook with complete code
â”‚â”€â”€ ğŸ“„ README.md                    # Project documentation
â”‚â”€â”€ ğŸ“„ requirements.txt              # List of dependencies
â”‚â”€â”€ ğŸ“‚ dataset/                      # Folder containing images and labels for training
â”‚â”€â”€ ğŸ“‚ models/                        # Pre-trained and fine-tuned YOLO models
â”‚â”€â”€ ğŸ“‚ results/                       # Output images with detected buttons
â”‚â”€â”€ ğŸ“‚ scripts/                       # Python scripts for training and inference
â”‚â”€â”€ ğŸ“‚ examples/                      # Sample test images
```

---

## **ğŸ› ï¸ Setup & Installation**
### **1ï¸âƒ£ Install Dependencies**
Make sure you have Python 3.8+ installed. Then, install the required packages using:

```bash
pip install -r requirements.txt
```

Alternatively, install the core dependencies manually:

```bash
pip install ultralytics easyocr opencv-python matplotlib numpy subprocess
```

> If you plan to fine-tune YOLO, **GPU acceleration is recommended** (NVIDIA CUDA for PyTorch).

---

## **ğŸ“œ How It Works**
### **ğŸ”¹ Step 1: Load an Image**
The script takes an image as input and preprocesses it:

```python
orig_image, rgb_image = preprocess_image("example.jpg")
```

### **ğŸ”¹ Step 2: Detect UI Buttons Using YOLO**
The **YOLO model** detects UI elements (such as buttons):

```python
yolo_detections = detect_buttons_yolo(rgb_image)
```
If no objects are detected, it prints:

```
âš  YOLO Raw Detections: 0
âœ… YOLO Filtered Detections: 0
```

To improve detection:
- Lower the confidence threshold in `detect_buttons_yolo()`.
- Fine-tune YOLO (explained below).

### **ğŸ”¹ Step 3: Extract Text Using EasyOCR**
The **EasyOCR model** extracts all readable text from the image:

```python
ocr_results = extract_text_easyocr(rgb_image)
```

Example output:

```
OCR Extracted Text: ["Not Now", "Cancel", "OK", "Settings"]
```

### **ğŸ”¹ Step 4: Ask LLaVA to Identify the Close Button**
The system **queries a language model (LLaVA)** to determine the correct button to click:

```python
chosen_text = query_llava_for_button(candidate_texts)
```

Example query:

```
We have extracted the following button texts from a pop-up:
["Not Now", "Cancel", "OK", "Settings"]
Which one is the button that closes or dismisses the pop-up?
```

Expected response:
```
LLaVA Selected: "Not Now"
```

### **ğŸ”¹ Step 5: Find the Clickable Button**
Once LLaVA selects the button, the system **matches the selected text** with OCR results using fuzzy matching:

```python
selected_candidate = fuzzy_match_candidate(candidates, chosen_text)
```

If a match is found, it extracts the button **bounding box** and calculates the **click position**.

### **ğŸ”¹ Step 6: Calculate the Click Coordinates**
The exact **center coordinates** of the detected button are computed:

```python
x1, y1, x2, y2 = selected_candidate['bbox']
click_x = (x1 + x2) / 2.0
click_y = (y1 + y2) / 2.0
norm_x = click_x / orig_image.shape[1]
norm_y = click_y / orig_image.shape[0]

print(f"ğŸ–± Click Coordinates: ({click_x}, {click_y}) | Normalized: ({norm_x:.2f}, {norm_y:.2f})")
```

Example output:

```
ğŸ–± Click Coordinates: (274, 975) | Normalized: (0.35, 0.57)
```

### **ğŸ”¹ Step 7: Visualize the Detected Buttons**
The system draws bounding boxes on the image and saves the output.

```python
output_image = draw_all_detections(orig_image, ocr_results, yolo_detections, selected_candidate)
cv2.imwrite("output_N_example.jpg", output_image)
```

---

## **ğŸ› ï¸ Fine-Tuning YOLO for Better Accuracy**
If YOLO is **not detecting buttons correctly**, fine-tuning it on a **custom dataset** will improve results.

### **ğŸ”¹ 1. Prepare the Dataset**
Create a folder **`dataset/`** and add labeled images in YOLO format:
```
dataset/
â”‚â”€â”€ images/
â”‚   â”‚â”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ image1.jpg
â”‚   â”‚   â”œâ”€â”€ image2.jpg
â”‚   â”‚â”€â”€ val/
â”‚   â”‚   â”œâ”€â”€ image3.jpg
â”‚   â”‚   â”œâ”€â”€ image4.jpg
â”‚â”€â”€ labels/
â”‚   â”‚â”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ image1.txt
â”‚   â”‚   â”œâ”€â”€ image2.txt
â”‚   â”‚â”€â”€ val/
â”‚   â”‚   â”œâ”€â”€ image3.txt
â”‚   â”‚   â”œâ”€â”€ image4.txt
```

### **ğŸ”¹ 2. Create the Dataset YAML File**
Save this as `popups.yaml`:
```yaml
train: dataset/images/train
val: dataset/images/val
names:
  0: close_button
```

### **ğŸ”¹ 3. Train YOLO with Your Dataset**
```python
from ultralytics import YOLO

# Load a pre-trained YOLO model
model = YOLO('yolo11x.pt')

# Train the model on custom dataset
model.train(data='popups.yaml', 
            epochs=50, 
            batch=16, 
            imgsz=640)

print("Training complete!")
```

### **ğŸ”¹ 4. Evaluate the Fine-Tuned Model**
```python
# Evaluate model performance
metrics = model.val(data='popups.yaml')
print(metrics)
```

### **ğŸ”¹ 5. Use Fine-Tuned Model for Inference**
```python
# Load the fine-tuned model
model = YOLO('runs/train/exp/weights/best.pt')

# Detect buttons in a new image
results = model("example.jpg")
results.show()
```

---

## **ğŸ” Future Improvements**
âœ… Improve YOLO model with more data.  
âœ… Use **LoRA** to fine-tune LLaVA to reduce mistakes.  
âœ… Add **multi-modal reasoning** to improve decision accuracy.

---

## **ğŸ“œ License**
This project is open-source under the **MIT License**.  

---

## **ğŸ‘¨â€ğŸ’» Author**
**Developed by [Your Name]**  
Contact: [Your Email]  
GitHub: [Your GitHub Profile]  

---

## **ğŸŒŸ Support & Contributions**
If you find this project useful, **please give it a â­ on GitHub**! Contributions are welcome.
